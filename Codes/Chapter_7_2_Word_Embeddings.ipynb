{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 7.2 - Word Embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUzQcorFS20I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dropout, Dense, Flatten, GlobalMaxPooling1D, Embedding, Conv1D, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6DC5sNxi85W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "07968a47-d4de-466a-c3be-4edb9ab855b6"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x46fXVvEhFvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = [\n",
        "\n",
        "\n",
        "    'This movie is excellent',\n",
        "    'I loved the movie, it was fantastic',\n",
        "    'The film is brilliant, you should watch',\n",
        "    'Wonderful movie',\n",
        "    'one of the best films ever',\n",
        "    'fantastic film to watch',\n",
        "    'great movie',\n",
        "    'Acting and direction is brilliant',\n",
        "\n",
        "\n",
        "\n",
        "    \"poor acting\",\n",
        "    'horrible film',\n",
        "    'pathetic acting',\n",
        "    'The film is very boring',\n",
        "    'I wasted my money',\n",
        "    'I did not like the film',\n",
        "    'not recommended',\n",
        "    'it was a poor story'\n",
        "]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXM-2DmQhxcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = np.array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRZTOA_Jh1TG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer .fit_on_texts(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_kk2bMjh-Fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary_length = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKjlCgz5iDa0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "04ed8f4f-e932-40b4-f61c-3718f5cd9311"
      },
      "source": [
        "integer_sentences = tokenizer.texts_to_sequences(dataset)\n",
        "print(integer_sentences)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14, 3, 4, 15], [5, 16, 1, 3, 7, 8, 9], [1, 2, 4, 10, 17, 18, 11], [19, 3], [20, 21, 1, 22, 23, 24], [9, 2, 25, 11], [26, 3], [6, 27, 28, 4, 10], [12, 6], [29, 2], [30, 6], [1, 2, 4, 31, 32], [5, 33, 34, 35], [5, 36, 13, 37, 1, 2], [13, 38], [7, 8, 39, 12, 40]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9K0Co0yiJO1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "6a1c1a25-a8c2-4fc4-88ec-4da6f29040ed"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "token_count = lambda sentence: len(word_tokenize(sentence))\n",
        "max_sentence = max(dataset, key = token_count)\n",
        "max_sentence_length = len(word_tokenize(max_sentence))\n",
        "\n",
        "padded_reviews = pad_sequences(integer_sentences , max_sentence_length, padding='post')\n",
        "\n",
        "print(padded_reviews)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14  3  4 15  0  0  0  0]\n",
            " [ 5 16  1  3  7  8  9  0]\n",
            " [ 1  2  4 10 17 18 11  0]\n",
            " [19  3  0  0  0  0  0  0]\n",
            " [20 21  1 22 23 24  0  0]\n",
            " [ 9  2 25 11  0  0  0  0]\n",
            " [26  3  0  0  0  0  0  0]\n",
            " [ 6 27 28  4 10  0  0  0]\n",
            " [12  6  0  0  0  0  0  0]\n",
            " [29  2  0  0  0  0  0  0]\n",
            " [30  6  0  0  0  0  0  0]\n",
            " [ 1  2  4 31 32  0  0  0]\n",
            " [ 5 33 34 35  0  0  0  0]\n",
            " [ 5 36 13 37  1  2  0  0]\n",
            " [13 38  0  0  0  0  0  0]\n",
            " [ 7  8 39 12 40  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfjW8XXsigox",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83d19d80-3d5c-477c-e13a-15b0035270e0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KuDjYBljXkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "embedd_dict= dict()\n",
        "glove_embeddings = open('/gdrive/My Drive/datasets/glove.6B.100d.txt', encoding=\"utf8\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj9-zbaJjwg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for embeddings in glove_embeddings:\n",
        "    embedding_tokens = embeddings.split()\n",
        "    emb_word = embedding_tokens [0]\n",
        "    emb_vector = asarray(   embedding_tokens[1:], dtype='float32')\n",
        "    embedd_dict [emb_word] = emb_vector \n",
        "\n",
        "glove_embeddings.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOC3dUTikHJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedd_mat= zeros((vocabulary_length, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embedd_dict.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedd_mat[index] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ89R-BIklK2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c9dc0937-a25d-4fa5-8f64-210756df494e"
      },
      "source": [
        "embedd_mat.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_CnIPLJkvA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_inputs = Input(shape=(max_sentence_length))\n",
        "embedding_layer = Embedding(vocabulary_length, 100, weights=[embedd_mat], trainable=False)(embedding_inputs)\n",
        "flatten_layer = Flatten()(embedding_layer)\n",
        "output_layer = Dense(1, activation='sigmoid')(flatten_layer)\n",
        "model = Model(inputs=embedding_inputs, outputs=output_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tOYR5-8lAvC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "24f36e34-8e7c-4750-fa2a-d552f3e9e851"
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model.summary())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 8)]               0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 8, 100)            4100      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 801       \n",
            "=================================================================\n",
            "Total params: 4,901\n",
            "Trainable params: 801\n",
            "Non-trainable params: 4,100\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK7Sf7aFlSbB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a1711ef-ee26-4060-e92f-2352a4f97a44"
      },
      "source": [
        "model.fit(padded_reviews, labels, epochs=100, verbose=1)\n",
        "loss, accuracy = model.evaluate(padded_reviews, labels, verbose=0)\n",
        "\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.9458 - acc: 0.5000\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8986 - acc: 0.5000\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8539 - acc: 0.5000\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8119 - acc: 0.5000\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7729 - acc: 0.5000\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7370 - acc: 0.5000\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7042 - acc: 0.5000\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6746 - acc: 0.5625\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6481 - acc: 0.6250\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6246 - acc: 0.6250\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6037 - acc: 0.7500\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5852 - acc: 0.7500\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5688 - acc: 0.8125\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5540 - acc: 0.8125\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5405 - acc: 0.8125\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5281 - acc: 0.7500\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5164 - acc: 0.8125\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5052 - acc: 0.8125\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4944 - acc: 0.8125\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4838 - acc: 0.8125\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4733 - acc: 0.8125\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4630 - acc: 0.8125\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4527 - acc: 0.8750\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4425 - acc: 0.9375\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4324 - acc: 0.9375\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4225 - acc: 0.9375\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4128 - acc: 0.9375\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4033 - acc: 0.9375\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3940 - acc: 0.9375\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3851 - acc: 0.9375\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3764 - acc: 0.9375\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3681 - acc: 0.9375\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3601 - acc: 1.0000\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3524 - acc: 1.0000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3450 - acc: 1.0000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3379 - acc: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3311 - acc: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3246 - acc: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3183 - acc: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3122 - acc: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3064 - acc: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3007 - acc: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2952 - acc: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2899 - acc: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2847 - acc: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2796 - acc: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2747 - acc: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2699 - acc: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2652 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2606 - acc: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2561 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2517 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2475 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2433 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2392 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2353 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2314 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2277 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2240 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2204 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2169 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2135 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2102 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2070 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2038 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2007 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1977 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1948 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1919 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1891 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1864 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1837 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1811 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1785 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1760 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1736 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1712 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1688 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1665 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1643 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1621 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1599 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1578 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1557 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1537 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1517 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1498 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1479 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1460 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1441 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1423 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1406 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1388 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1371 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1355 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1339 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1322 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1307 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1291 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1276 - acc: 1.0000\n",
            "Accuracy: 100.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c53go0uzlkOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
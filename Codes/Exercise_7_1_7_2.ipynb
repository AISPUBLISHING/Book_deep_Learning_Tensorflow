{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Kqbv9F0osEP"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xuvMyUoqx2A"
   },
   "source": [
    " ## Exercise 7.1\n",
    "\n",
    " ### Question 1\n",
    "\n",
    " Which of the following is not a pre trained word embedding\n",
    "\n",
    " 1. Glove\n",
    " 2. Stanford\n",
    " 3. Peeking\n",
    " 4. All of the above\n",
    "\n",
    "### Answer: 3 \n",
    "\n",
    "\n",
    "### Question 2\n",
    " \n",
    " What should be the first argument to the Keras Embedding Layer.\n",
    "\n",
    "1. The input vector dimensions\n",
    "2. The output vector dimensions\n",
    "3. The word embedding size\n",
    "4. The vocabulary size\n",
    "\n",
    "### Answer: 4\n",
    "\n",
    "### Question 3\n",
    "Which layer will you need to use if you want to directly connect the Embedding Layer with a Dense Layer\n",
    "\n",
    "1. LSTM layer with return_sequence set to False\n",
    "2. CNN layer with max pooling\n",
    "3. Flatten layer\n",
    "4. None of the Above\n",
    "\n",
    "### Answer: 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFbbX2G2q13-"
   },
   "source": [
    "## Exercise 7.2\n",
    "\n",
    "Using the IMDB dataset that we used for sentiment classification in section 7.4, perform sentiment classification using an RNN.\n",
    "See if you can get better results with RNN as compared to the results in section 7.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QkN06azHpa_B"
   },
   "source": [
    "### Solution\n",
    "\n",
    "This is only a basic solution, you can add or remove RNN or DNN layers if you want along with the dropout sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "4s-uJ0ujpEMQ",
    "outputId": "545da86e-745a-47da-c191-0f0bb7cc166e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
      "Epoch 1/10\n",
      "250/250 [==============================] - 112s 449ms/step - loss: 19787.7559 - accuracy: 0.5923 - val_loss: 0.6327 - val_accuracy: 0.6505\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 112s 448ms/step - loss: 0.6441 - accuracy: 0.6342 - val_loss: 0.6369 - val_accuracy: 0.6400\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 113s 450ms/step - loss: 0.6387 - accuracy: 0.6330 - val_loss: 0.6268 - val_accuracy: 0.6503\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 115s 461ms/step - loss: 0.6338 - accuracy: 0.6378 - val_loss: 0.6219 - val_accuracy: 0.6509\n",
      "Epoch 5/10\n",
      "164/250 [==================>...........] - ETA: 35s - loss: 0.6314 - accuracy: 0.6399"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, LSTM, Dropout, Dense, Flatten, Input,  Embedding, Conv1D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "imdb_data= pd.read_csv(\"/gdrive/My Drive/datasets/IMDB Dataset.csv\")\n",
    "\n",
    "imdb_data.head()\n",
    "\n",
    "imdb_data.sentiment.value_counts()\n",
    "X = imdb_data[\"review\"]\n",
    "\n",
    "y = pd.get_dummies(imdb_data.sentiment, prefix='sent', drop_first=True).values\n",
    "y.shape\n",
    "def clean_text(doc):\n",
    "\n",
    "    document = remove_tags(doc)\n",
    "\n",
    "    document = re.sub('[^a-zA-Z]', ' ', document)\n",
    "\n",
    "    document = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', document)\n",
    "\n",
    "    document = re.sub(r'\\s+', ' ', document)\n",
    "\n",
    "    return document\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(document):\n",
    "    return TAG_RE.sub('', document)\n",
    "X_sentences = []\n",
    "reviews = list(X)\n",
    "for rev in reviews:\n",
    "    X_sentences.append(clean_text(rev))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sentences, y, test_size=0.20, random_state=42)\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embedd_dict= dict()\n",
    "glove_embeddings = open('/gdrive/My Drive/datasets/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for embeddings in glove_embeddings:\n",
    "    embedding_tokens = embeddings.split()\n",
    "    emb_word = embedding_tokens [0]\n",
    "    emb_vector = asarray(   embedding_tokens[1:], dtype='float32')\n",
    "    embedd_dict [emb_word] = emb_vector \n",
    "\n",
    "glove_embeddings.close()\n",
    "embedd_mat= zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embedd_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedd_mat[index] = embedding_vector\n",
    "embedd_mat.shape\n",
    "\n",
    "\n",
    "embedding_inputs = Input(shape=(maxlen))\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedd_mat], trainable=False)(embedding_inputs )\n",
    "lstm1 = LSTM(128, activation='relu', return_sequences=True)(embedding_layer)\n",
    "\n",
    "lstm2 = LSTM(64, activation='relu', )(lstm1)\n",
    "\n",
    "drop1 = Dropout(0.2)(lstm2)\n",
    "dense1 = Dense(512, activation = 'relu')(drop1)\n",
    "drop2  = Dropout(0.2)(dense1)\n",
    "output_layer = Dense(1, activation= 'sigmoid')(drop2)\n",
    "\n",
    "model = Model(embedding_inputs, output_layer )\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model_plot1.png', show_shapes=True, show_layer_names=True)\n",
    "history = model.fit(X_train, y_train, batch_size= 128, epochs=10, verbose=1, validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(score[0])\n",
    "print(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SxNg-i983yXM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise 7.1 - 7.2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
